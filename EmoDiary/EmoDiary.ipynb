{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuU9EJ3vk2yO"
      },
      "source": [
        "\n",
        "***EmoDiary : 스마트 감정 일기 도우미 서비스***\n",
        "\n",
        "https://huggingface.co/spaces/dazzleun-7/Bigdatacapstone_24-2\n",
        "\n",
        "- [부제 1] 감정 실시간 감지를 통한 맞춤형 플레이리스트 추천\n",
        "- [부제 2] 일기 회고 콘텐츠 추천\n",
        "---\n",
        "- 오늘 하루 일상을 표현하는 얼굴 표정 + 텍스트를 받음\n",
        "- 1)감정과 유사한 or 반대되는 음악을 추천해주고\n",
        "- 2)생성형 AI는\n",
        "    - 일기 모멘텀의 역할 : 지속적으로 일기를 작성할 수 있도록 동기부여와 흐름을 제공\n",
        "       - 사용자의 감정 벡터를 기반으로 감정 캐릭터 이미지 제공\n",
        "       - 사용자가 남긴 하루 기록에 대해 답장 제공\n",
        "       - 내면을 들여다볼 수 있는 구체적인 회고 콘텐츠 제공\n",
        "---\n",
        "\n",
        "*  일기 쓰기는 개인이 자신의 생각과 감정을 깊이 탐구하고 이를 글로 표현함으로써 자기 성찰을 촉구하는 강력한 도구로 널리 인정받고 있음. 특히, 정서적으로 복잡한 상황을 글로 정리하는 과정에서 개인은 자신을 객관적으로 이해하고, 정서적 안정을 도모할 수 있음\n",
        "\n",
        "    - (기타) 하지만, 현재 일기를 쓸 때 느끼는 어려운 점\n",
        "        - 일기를 통한 감정 파악의 어려움\n",
        "        - 일기의 소재를 떠올리기 어려움\n",
        "        - 글을 작성하는 것에 대한 부담감\n",
        "\n",
        "* 음악 감상은 개인의 정서 조절에 효과적이며, 자기 성찰 과정에서 감정 인식을 돕는 역할을 한다는 연구 결과가 보고됨. (https://s-space.snu.ac.kr/handle/10371/120423?utm_source=chatgpt.com)\n",
        "\n",
        "* 이러한 이론적 근거를 토대로 본 프로젝트는 사용자 감정 분석을 기반으로 한\n",
        "음악과 일기 작성 간의 심리적 효과를 연계하는 기술적 접근을 제안하여 사용자의 정서적 건강 증진과 자기 성찰을 지원할 수 있도록 한다. 또한 생성형 AI를 통해 단순히 '오늘의 기록'을 넘어 감정적 연결을 끌어내고, 회고 가이드라인을 제공하여 글을 쓴다는 것에 대한 부담감을 줄이며 자신을 더 잘 이해할 수 있도록 도움\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BkgfHmYznOZ"
      },
      "source": [
        "#1.Colab 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHYYMgCUAtk-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mDd7qfzJuH_4"
      },
      "outputs": [],
      "source": [
        "#필요 패키지 설치\n",
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install tqdm pandas\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install numpy==1.23.1\n",
        "\n",
        "#KoBERT 깃허브에서 불러오기\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "\n",
        "!pip install langchain==0.0.125 chromadb==0.3.14 pypdf==3.7.0 tiktoken==0.3.3\n",
        "!pip install openai==0.28\n",
        "!pip install gradio transformers torch opencv-python-headless\n",
        "\n",
        "#gradio\n",
        "!pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYGG89q1zrT-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "\n",
        "#  Hugging Face를 통한 모델 및 토크나이저 Import\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKXEqbjHBxBW"
      },
      "outputs": [],
      "source": [
        "n_devices = torch.cuda.device_count()\n",
        "print(n_devices)\n",
        "\n",
        "for i in range(n_devices):\n",
        "    print(torch.cuda.get_device_name(i))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FzuB_WNzf0S"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SsE0JVzefEQk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/kobert.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrNUrrpElVgc"
      },
      "outputs": [],
      "source": [
        "#슬픔&상처&불안 -> 우울로 변경\n",
        "df.loc[df['감정_대분류'] == '슬픔', '감정_대분류'] = '우울'\n",
        "df.loc[df['감정_대분류'] == '상처', '감정_대분류'] = '우울'\n",
        "df.loc[df['감정_대분류'] == '불안', '감정_대분류'] = '우울'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa8klFPdp9lP"
      },
      "outputs": [],
      "source": [
        "# 당황 데이터 제거\n",
        "df = df[df['감정_대분류'] != '당황'].reset_index(drop=True)\n",
        "df['감정_대분류'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSLk62KXt8ZP"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index(drop=True)\n",
        "df = df[['감정_대분류', '사람문장1']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoZYD1ss1vwZ"
      },
      "outputs": [],
      "source": [
        "df['감정_대분류'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYJj0K0j1Jkj"
      },
      "outputs": [],
      "source": [
        "# 6개의 감정 class → 숫자\n",
        "df.loc[(df['감정_대분류'] == \"기쁨\"), '감정_대분류'] = 0  # 기쁨 → 0\n",
        "df.loc[(df['감정_대분류'] == \"즐거움\"), '감정_대분류'] = 1  # 즐거움 → 1\n",
        "df.loc[(df['감정_대분류'] == \"사랑\"), '감정_대분류'] = 2  # 사랑 → 2\n",
        "df.loc[(df['감정_대분류'] == \"분노\"), '감정_대분류'] = 3  # 분노 → 3\n",
        "df.loc[(df['감정_대분류'] == \"우울\"), '감정_대분류'] = 4  # 우울 → 4\n",
        "df.loc[(df['감정_대분류'] == \"외로움\"), '감정_대분류'] = 5  # 외로움 → 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_dQOOZP1mSQ"
      },
      "outputs": [],
      "source": [
        "df['감정_대분류'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ad7nPuK24C6"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for q, label in zip(df['사람문장1'], df['감정_대분류'])  : ## BERTDataset에 input으로 주기 위해 문장과 감정라벨로 이루어진 list append\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6QLaouM3TiQ"
      },
      "outputs": [],
      "source": [
        "print(len(data_list))\n",
        "print(data_list[0])\n",
        "print(data_list[140])\n",
        "print(data_list[1534])\n",
        "print(data_list[3000])\n",
        "print(data_list[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPGqxpEjkWWp"
      },
      "outputs": [],
      "source": [
        "df[250:600]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8eDT5n33dCX"
      },
      "source": [
        "#3.데이터 분리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRamW1EuVgp8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)\n",
        "\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGbd6lQrtfkQ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# dataset_train에서 레이블(두 번째 항목)을 추출하여 카운트\n",
        "labels_train = [item[1] for item in dataset_train]\n",
        "label_counts_train = Counter(labels_train)\n",
        "\n",
        "# 레이블 분포 출력 --> train 데이터 내부에서 데이터 편향 문제 발생\n",
        "print(\"Train 데이터 레이블 분포:\")\n",
        "print(label_counts_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPWyZoYD_yEL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# 언더샘플링 대상 레이블 설정 및 목표 샘플 수 정의\n",
        "target_counts = {'4': 6000, '3': 4000, '0': 4000}\n",
        "\n",
        "# 언더샘플링 적용할 데이터셋\n",
        "final_dataset_train = []\n",
        "\n",
        "# 레이블별로 데이터를 그룹화\n",
        "data_by_label = {label: [] for label in target_counts.keys()}\n",
        "\n",
        "# 레이블별로 데이터를 분류\n",
        "for sentence, label in dataset_train:\n",
        "    if label in target_counts:\n",
        "        data_by_label[label].append((sentence, label))\n",
        "\n",
        "# 다수 클래스는 목표 샘플 수만큼 무작위 샘플링\n",
        "for label, items in data_by_label.items():\n",
        "    target_count = target_counts[label]\n",
        "    sampled_items = random.sample(items, target_count) if len(items) > target_count else items\n",
        "    final_dataset_train.extend(sampled_items)\n",
        "\n",
        "# 소수 클래스는 그대로 추가\n",
        "for sentence, label in dataset_train:\n",
        "    if label not in target_counts:\n",
        "        final_dataset_train.append((sentence, label))\n",
        "\n",
        "print(\"Undersampled Train Dataset Size:\", len(final_dataset_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k4Z--Y5ESwh"
      },
      "outputs": [],
      "source": [
        "# dataset_train에서 레이블(두 번째 항목)을 추출하여 카운트\n",
        "labels_train = [item[1] for item in final_dataset_train]\n",
        "label_counts_train = Counter(labels_train)\n",
        "\n",
        "# 레이블 분포 출력\n",
        "print(\"Train 데이터 레이블 분포:\")\n",
        "print(label_counts_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9L2mY4jpDIA"
      },
      "outputs": [],
      "source": [
        "labels_train_series = pd.Series(labels_train)\n",
        "class_counts = labels_train_series.value_counts().to_dict()  # 딕셔너리 형태로 변경\n",
        "num_samples = sum(class_counts.values())  # 총 샘플 수\n",
        "\n",
        "# 클래스별 가중치 부여 (딕셔너리 형태로)\n",
        "class_weights = {label: num_samples / count for label, count in class_counts.items()}\n",
        "\n",
        "# 해당 데이터의 label에 해당되는 가중치 부여\n",
        "weights = [class_weights[label] for label in labels_train]\n",
        "\n",
        "# WeightedRandomSampler 정의\n",
        "sampler = WeightedRandomSampler(weights=torch.DoubleTensor(weights), num_samples=len(weights))\n",
        "\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqr04ZvX5M4i"
      },
      "source": [
        "# 4. 데이터 변환 (토큰화, 정수 인코딩, 패딩)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIkHpedEOQ-2"
      },
      "outputs": [],
      "source": [
        "## Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 32\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMfYKdX8OSUx"
      },
      "outputs": [],
      "source": [
        "## BERT 스타일의 데이터 변환을 수행하는 클래스\n",
        "## BERT 모델에 입력되는 문장 또는 문장 쌍을 적절한 형식으로 변환해 모델이 이해할 수 있도록 함\n",
        "\n",
        "\n",
        "class BERTSentenceTransform:\n",
        "    r\"\"\"BERT style data transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : BERTTokenizer.\n",
        "        Tokenizer for the sentences.\n",
        "    max_seq_length : int.\n",
        "        Maximum sequence length of the sentences.\n",
        "    pad : bool, default True\n",
        "        Whether to pad the sentences to maximum length.\n",
        "    pair : bool, default True\n",
        "        Whether to transform sentences or sentence pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    # 입력으로 받은 tokenizerm 최대 시퀀스 길이, vocab, pad 및 pair 설정\n",
        "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
        "        self._tokenizer = tokenizer\n",
        "        self._max_seq_length = max_seq_length\n",
        "        self._pad = pad\n",
        "        self._pair = pair\n",
        "        self._vocab = vocab\n",
        "\n",
        "    # 입력된 문장 또는 문장 쌍을 BERT 모델이 사용할 수 있는 형식으로 변환\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
        "\n",
        "        The transformation is processed in the following steps:\n",
        "        - tokenize the input sequences\n",
        "        - insert [CLS], [SEP] as necessary\n",
        "        - generate type ids to indicate whether a token belongs to the first\n",
        "        sequence or the second sequence.\n",
        "        - generate valid length\n",
        "\n",
        "        For sequence pairs, the input is a tuple of 2 strings:\n",
        "        text_a, text_b.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'is this jacksonville ?'\n",
        "            text_b: 'no it is not'\n",
        "        Tokenization:\n",
        "            text_a: 'is this jack ##son ##ville ?'\n",
        "            text_b: 'no it is not .'\n",
        "        Processed:\n",
        "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
        "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "            valid_length: 14\n",
        "\n",
        "        For single sequences, the input is a tuple of single string:\n",
        "        text_a.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Tokenization:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Processed:\n",
        "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
        "            type_ids: 0     0   0   0  0     0 0\n",
        "            valid_length: 7\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        line: tuple of str\n",
        "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
        "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
        "            string: (text_a,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: valid length in 'int32', shape (batch_size,)\n",
        "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert to unicode\n",
        "        text_a = line[0]\n",
        "        if self._pair:\n",
        "            assert len(line) == 2\n",
        "            text_b = line[1]\n",
        "\n",
        "        tokens_a = self._tokenizer.tokenize(text_a)\n",
        "        tokens_b = None\n",
        "\n",
        "        if self._pair:\n",
        "            tokens_b = self._tokenizer(text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
        "                                    self._max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > self._max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
        "\n",
        "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
        "        # pre-training and are added to the wordpiece embedding vector\n",
        "        # (and position vector). This is not *strictly* necessary since\n",
        "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        #vocab = self._tokenizer.vocab\n",
        "        vocab = self._vocab\n",
        "        tokens = []\n",
        "        tokens.append(vocab.cls_token)\n",
        "        tokens.extend(tokens_a)\n",
        "        tokens.append(vocab.sep_token)\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens.extend(tokens_b)\n",
        "            tokens.append(vocab.sep_token)\n",
        "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The valid length of sentences. Only real  tokens are attended to.\n",
        "        valid_length = len(input_ids)\n",
        "\n",
        "        if self._pad:\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = self._max_seq_length - valid_length\n",
        "            # use padding tokens for the rest\n",
        "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
        "            segment_ids.extend([0] * padding_length)\n",
        "\n",
        "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
        "            np.array(segment_ids, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq_MeHo-OWT8"
      },
      "outputs": [],
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "## 주어진 데이터셋을 KoBERT 모델에 입력으로 사용할 수 있는 형식으로 변환 후 이를 토대로 텍스트 분류 작업을 수행할 수 있도록 함\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "        #transform = nlp.data.BERTSentenceTransform(\n",
        "        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
        "\n",
        "\n",
        "data_train = BERTDataset(final_dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "\n",
        "# KoBERTTokenizer를 사용해 한국어 BERT용 토크나이저와 BertModle을 사용해 사전 학습된 KoBERT 모델 불러옴\n",
        "# nlp.vocab 사용해 BERT 모델의 어휘 불러옴\n",
        "# 학습 및 테스트 데이터셋을 BERTDataset 클래스를 사용해 준비 --> 각각의 문장은 토큰화되고 어휘에 따라 숫자로 인덱싱됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IlGLv3155bu-"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=2,sampler=sampler, shuffle=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax6oEA1yoOSC"
      },
      "source": [
        "#5-1. Kobert_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huqeRWMDoWs7"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size=768,\n",
        "                 num_classes=6,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "        self.softmax = nn.Softmax(dim=1)  # Softmax로 변경\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=hidden_size, out_features=512),\n",
        "            nn.Linear(in_features=512, out_features=num_classes),\n",
        "        )\n",
        "\n",
        "        # 정규화 레이어 추가 (Layer Normalization)\n",
        "        self.layer_norm = nn.LayerNorm(768)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n",
        "\n",
        "        pooled_output = self.dropout(pooler)\n",
        "        normalized_output = self.layer_norm(pooled_output)\n",
        "        out = self.classifier(normalized_output)\n",
        "\n",
        "        # LayerNorm 적용\n",
        "        pooler = self.layer_norm(pooler)\n",
        "\n",
        "        if self.dr_rate:\n",
        "            pooler = self.dropout(pooler)\n",
        "\n",
        "        logits = self.classifier(pooler)  # 분류를 위한 로짓 값 계산\n",
        "        probabilities = self.softmax(logits)  # Softmax로 각 클래스의 확률 계산\n",
        "        return probabilities  # 각 클래스에 대한 확률 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F6PahTmoWqs"
      },
      "outputs": [],
      "source": [
        "#정의한 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,dr_rate=0.4).to(device)\n",
        "#model = BERTClassifier(bertmodel,  dr_rate=0.5).to('cpu')\n",
        "\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ijwhvJNBI1C"
      },
      "source": [
        "# 6. 모델 학습 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TVw3P1efvrQZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 각 에포크의 F1 스코어를 저장할 리스트 초기화\n",
        "train_f1_scores = []\n",
        "test_f1_scores = []\n",
        "\n",
        "# 모델 훈련 및 평가\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    total_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "    all_test_preds = []\n",
        "    all_test_labels = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies =[]\n",
        "    avg_test_acc=0.0\n",
        "    model.train()\n",
        "\n",
        "    # 훈련 데이터 학습\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        total_loss += loss.item()\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "\n",
        "        # 예측값과 실제 라벨 저장\n",
        "        _, preds = torch.max(out, dim=1)\n",
        "        all_train_preds.extend(preds.cpu().numpy())\n",
        "        all_train_labels.extend(label.cpu().numpy())\n",
        "\n",
        "    # 에포크별 평균 손실 및 정확도 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    avg_train_acc = train_acc / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_acc)\n",
        "\n",
        "    # F1 스코어 계산\n",
        "    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
        "    train_f1_scores.append(train_f1)\n",
        "\n",
        "    print(f\"epoch {e+1} train loss {avg_train_loss} train acc {avg_train_acc} train f1 {train_f1}\")\n",
        "\n",
        "    # 검증 데이터 평가\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "\n",
        "        # 예측값과 실제 라벨 저장\n",
        "        _, preds = torch.max(out, dim=1)\n",
        "        all_test_preds.extend(preds.cpu().numpy())\n",
        "        all_test_labels.extend(label.cpu().numpy())\n",
        "\n",
        "    # 에포크별 테스트 정확도 및 F1 스코어 계산\n",
        "    avg_test_acc = test_acc / len(test_dataloader)\n",
        "    test_accuracies.append(avg_test_acc)\n",
        "    test_f1 = f1_score(all_test_labels, all_test_preds, average='macro')\n",
        "    test_f1_scores.append(test_f1)\n",
        "\n",
        "    print(f\"epoch {e+1} test acc {avg_test_acc} test f1 {test_f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw4qb-qJvtXK"
      },
      "outputs": [],
      "source": [
        "# 저장 경로 설정 (Google Drive 내 원하는 폴더로 변경 가능)\n",
        "#save_path = '/content/drive/MyDrive/model_weights_softmax.pth'\n",
        "\n",
        "# 모델 상태_dict 저장\n",
        "#torch.save(model.state_dict(), save_path)\n",
        "#print(f'Model saved to {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LglivkUPUBxJ"
      },
      "outputs": [],
      "source": [
        "# 모델 저장 경로\n",
        "model_save_path = '/content/drive/MyDrive/model_weights_softmax(model).pth'\n",
        "\n",
        "# 모델 전체 저장\n",
        "torch.save(model, model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTxSQ-2EAyih"
      },
      "source": [
        "# 7. 사용자 입력 문장\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vvJ6k4np9qx"
      },
      "outputs": [],
      "source": [
        "#테스트\n",
        "#토큰화\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "#tok = tokenizer.tokenize\n",
        "\n",
        "sentence_emotions = []\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model (token_ids, valid_length, segment_ids)\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            #emotions = [round(value.item() *100,2) for value in i]\n",
        "            emotions = [value.item() for value in i]\n",
        "            #print(emotions)\n",
        "            sentence_emotions.append(emotions)\n",
        "        print(sentence_emotions)\n",
        "\n",
        "#질문 무한반복하기! 0 입력시 종료\n",
        "sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "predict(sentence)\n",
        "#end = 1\n",
        "#while end == 1 :\n",
        " #   sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        " #   if sentence == '0' :\n",
        " #       break\n",
        " #   predict(sentence)\n",
        "    #print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-oCBQ--4BODo"
      },
      "outputs": [],
      "source": [
        "sentence_emotions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W9Iu_Cvhq1kh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69oXlTRJBX9L"
      },
      "source": [
        "# 8. 사용자 표정 인식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It_kI5HKEef0"
      },
      "source": [
        "### OpenCV 이용해서 실시간 사용자 이미지 데이터 받아오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lyKJorEmFgFd"
      },
      "outputs": [],
      "source": [
        "pip  install  git+https://github.com/openai/CLIP.git  scikit-image  matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T3E8jS5nrXSY"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPEVuQNEBzF3"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QpaNwjq7DtX-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Gya7eoFZW3"
      },
      "source": [
        "### Clip 모델 사용하여 image classification\n",
        "\n",
        "* 사전 훈련이나 클래스의 사전 레이블이 지정되지 않은 데이터여도 완벽한 이미지 분류 성능을 달성함 (Zero-shot)    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDMEwVF4GPcR"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import logging\n",
        "\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_w4xOoZ7HtNf"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-large-patch14\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bWwegd9kH-Gr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "print(f\"processor details for preprocessing: \\n {processor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdiHCcymITGY"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-eMyLeoIaqJ"
      },
      "outputs": [],
      "source": [
        "images = Image.open(\"/content/photo.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9nMFw9TIgKH"
      },
      "outputs": [],
      "source": [
        "labels = ['a photo of a happy face', 'a photo of a joyful face', 'a photo of a loving face', 'a photo of a angry face',  'a photo of a melancholic face',  'a photo of a lonely face']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm0_wJ0YIqaK"
      },
      "outputs": [],
      "source": [
        "inputs = processor(text = labels,\n",
        "                  images = images,\n",
        "                  return_tensors = \"pt\",\n",
        "                  padding = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A-MNukKcIrot"
      },
      "outputs": [],
      "source": [
        "print(f\"details of inputs: \\n {inputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CheUtDSCIsxx"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)\n",
        "print(f\"outputs: \\n {outputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6RAzAU0UIubl"
      },
      "outputs": [],
      "source": [
        "outputs.logits_per_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pBAUlsU5IxX3"
      },
      "outputs": [],
      "source": [
        "probs = outputs.logits_per_image.softmax(dim=1)[0]  # softmax로 변환\n",
        "\n",
        "print(f\"probs : {probs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbaYS5xLIyk0"
      },
      "outputs": [],
      "source": [
        "probs = list(probs)\n",
        "for i in range(len(labels)):\n",
        "  print(f\"label: {labels[i]} - probability of {probs[i].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opQsMp3oDB5Q"
      },
      "source": [
        "# 9. 최종 사용자 감정 벡터 출력 (텍스트 + 표정)\n",
        "\n",
        "\n",
        "*   텍스트 기반 감정 분석의 신뢰도가 안면 감정 분석보다 더 높다는 연구 결과를 반영하여 가중치를 각각 0.7과 0.3으로 설정함\n",
        "\n",
        "*   v combined =0.7*vtext+0.3*vimage​\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6gaSfcsDTpC"
      },
      "outputs": [],
      "source": [
        "# 얼굴 표정 기반 감정 벡터 추출\n",
        "image_emotions = [float(prob.item()) for prob in probs]\n",
        "image_emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrHLCKvkI2At"
      },
      "outputs": [],
      "source": [
        "# 텍스트 기반 감정 벡터 추출\n",
        "sentence_emotions = [item for sublist in sentence_emotions for item in sublist]\n",
        "sentence_emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbTMEhyGPlvW"
      },
      "outputs": [],
      "source": [
        "image_emotions = np.array(image_emotions)\n",
        "sentence_emotions = np.array(sentence_emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xybD8YXcJSsJ"
      },
      "outputs": [],
      "source": [
        "final_user_emotions = image_emotions * 0.3 + sentence_emotions * 0.7\n",
        "final_user_emotions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLMtZfyV7-S"
      },
      "source": [
        "#10. 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMGXCx2cXH9_"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/drive/MyDrive/model_weights_softmax(model).pth의 사본')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-jfn_m2xl7Q"
      },
      "source": [
        "#11. 멜론데이터 감정 벡터 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wdzi6heqnmIY"
      },
      "outputs": [],
      "source": [
        "# 전처리 전 멜론 데이터\n",
        "melon_data = pd.read_csv('/content/drive/MyDrive/melon_data.csv')\n",
        "melon_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YKhyM1GM_V4J"
      },
      "outputs": [],
      "source": [
        "# 전처리 + 감정 벡터로 표현된 멜론 데이터\n",
        "melon_emotions = pd.read_csv('/content/drive/MyDrive/melon_emotions_final.csv')\n",
        "melon_emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S_VEYXQ_yl9S"
      },
      "outputs": [],
      "source": [
        "melon_emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsfCx6-doVZ7"
      },
      "outputs": [],
      "source": [
        "melon_emotions = pd.merge(melon_emotions, melon_data, left_on='Title', right_on='title', how='inner')\n",
        "melon_emotions = melon_emotions[['singer', 'Title', 'genre','Emotions']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m5VpI5sOoyD0"
      },
      "outputs": [],
      "source": [
        "# [가수명, 곡명, 장르, 감정벡터]로 최종 정리된 멜론 데이터 프레임\n",
        "melon_emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAc-516GF1SM"
      },
      "outputs": [],
      "source": [
        "# 멜론 데이터의 Title에서 중복되는 노래 제목 하나만 남기고 제거\n",
        "melon_emotions = melon_emotions.drop_duplicates(subset='Title', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U8dPA5HPElZ9"
      },
      "outputs": [],
      "source": [
        "# 중복된 값이 포함된 행 전체 확인\n",
        "duplicate_rows = melon_emotions[melon_emotions['Title'].duplicated(keep=False)]\n",
        "print(duplicate_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSd2wk6SVl-E"
      },
      "source": [
        "#12. 코사인 유사도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBb8Euda73Xi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "melon_emotions['Emotions'] = melon_emotions['Emotions'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "emotions = melon_emotions['Emotions'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd9QNQDXAinD"
      },
      "outputs": [],
      "source": [
        "print(emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThhhaUFo8XOj"
      },
      "outputs": [],
      "source": [
        "print(final_user_emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rOFHLNZ4mzM"
      },
      "outputs": [],
      "source": [
        "print(type(final_user_emotions))\n",
        "print(type(emotions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXGlwPGo8VUI"
      },
      "outputs": [],
      "source": [
        "# 코사인 유사도 함수\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return np.nan  # 제로 벡터인 경우 NaN 반환\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "# 각 노래에 대한 코사인 유사도 계산\n",
        "similarities = [cosine_similarity(final_user_emotions, song_vec) for song_vec in emotions]\n",
        "\n",
        "# 유사도가 NaN이 아닌 인덱스만 필터링\n",
        "valid_indices = [i for i, sim in enumerate(similarities) if not np.isnan(sim)]\n",
        "filtered_similarities = [similarities[i] for i in valid_indices]\n",
        "\n",
        "recommendations = np.argsort(filtered_similarities)[::-1]  # 높은 유사도 순으로 인덱스 정렬\n",
        "print(recommendations)\n",
        "# 결과를 데이터프레임으로 만들기\n",
        "results_df = pd.DataFrame({\n",
        "    'Singer' : melon_emotions['singer'].iloc[recommendations].values,\n",
        "    'title' : melon_emotions['Title'].iloc[recommendations].values,\n",
        "    'genre' : melon_emotions['genre'].iloc[recommendations].values,\n",
        "    'Cosine Similarity': [similarities[idx] for idx in recommendations]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxQNbBkBhvEO"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15Hk-207TVfg"
      },
      "outputs": [],
      "source": [
        "results_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mflYl7teUJqN"
      },
      "outputs": [],
      "source": [
        "results_df.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0K84Ntv3ctj"
      },
      "source": [
        "# 13. 플레이리스트 1차 필터링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huh_06SWEh7M"
      },
      "outputs": [],
      "source": [
        "# 감정과 유사한 플레이리스트 (상위 5개)\n",
        "similar_playlists = results_df.head(5)\n",
        "\n",
        "# 감정과 반대되는 플레이리스트 (하위 5개)\n",
        "dissimilar_playlists = results_df.tail(5)\n",
        "\n",
        "# 사용자에게 선택지 보여주기\n",
        "print(\"내 감정과 유사한 플레이리스트 (상위 5개):\")\n",
        "print(similar_playlists)\n",
        "\n",
        "print(\"\\n내 감정과 반대되는 플레이리스트 (하위 5개):\")\n",
        "print(dissimilar_playlists)\n",
        "\n",
        "# 사용자 선호도 입력받기\n",
        "print(\"\\n어떤 플레이리스트를 듣고 싶으신가요?\")\n",
        "print(\"1. 내 감정과 유사한 플레이리스트\")\n",
        "print(\"2. 내 감정과 반대되는 플레이리스트\")\n",
        "choice = input(\"선택 (1 또는 2 입력): \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnlPmwnRLr5V"
      },
      "source": [
        "# 14. 플레이리스트 2차 필터링\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVvtG6rKO-SP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "# 가중치 값 설정\n",
        "gamma = 0.3\n",
        "\n",
        "# 초기 데이터 처리\n",
        "if choice == '1':\n",
        "    similar_playlists = pd.merge(similar_playlists, melon_emotions, left_on=\"title\", right_on=\"Title\", how=\"inner\")\n",
        "    similar_playlists = similar_playlists[[\"title\", \"Emotions\", \"singer\"]]\n",
        "\n",
        "    results = []\n",
        "    seen_songs = set(similar_playlists[\"title\"].values)  # 초기 seen_songs에 similar_playlists의 곡들을 추가\n",
        "\n",
        "    # 사용자 감정 벡터\n",
        "    user_emotion_vector = np.array(final_user_emotions).reshape(1, -1)\n",
        "\n",
        "    for index, row in similar_playlists.iterrows():\n",
        "        song_title = row[\"title\"]\n",
        "        song_singer = row[\"singer\"]\n",
        "        song_vector = np.array(row[\"Emotions\"]).reshape(1, -1)\n",
        "\n",
        "        song_results = []\n",
        "        for i, emotion_vec in enumerate(emotions):\n",
        "            emotion_title = melon_emotions.iloc[i][\"Title\"]\n",
        "            emotion_singer = melon_emotions.iloc[i][\"singer\"]\n",
        "            emotion_vec = np.array(emotion_vec).reshape(1, -1)\n",
        "\n",
        "            # similar_playlists에 있는 곡과 seen_songs에 있는 곡은 제외\n",
        "            if (\n",
        "                emotion_title != song_title and\n",
        "                emotion_title not in seen_songs\n",
        "            ):\n",
        "                try:\n",
        "                    # 곡 간 유사도(Song-Song Similarity)\n",
        "                    song_song_similarity = cosine_similarity(song_vector, emotion_vec)[0][0]\n",
        "\n",
        "                    # 사용자 감정 벡터와의 유사도(User-Song Similarity)\n",
        "                    user_song_similarity = cosine_similarity(user_emotion_vector, emotion_vec)[0][0]\n",
        "\n",
        "                    # Final Score 계산\n",
        "                    final_score = gamma * song_song_similarity + (1 - gamma) * user_song_similarity\n",
        "\n",
        "                    song_results.append({\n",
        "                        \"Title\": emotion_title,\n",
        "                        \"Singer\": emotion_singer,\n",
        "                        \"Song-Song Similarity\": song_song_similarity,\n",
        "                        \"User-Song Similarity\": user_song_similarity,\n",
        "                        \"Final Score\": final_score\n",
        "                    })\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error with {song_title} vs {emotion_title}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Final Score를 기준으로 상위 3곡 선택\n",
        "        song_results = sorted(song_results, key=lambda x: x[\"Final Score\"], reverse=True)[:3]\n",
        "        seen_songs.update([entry[\"Title\"] for entry in song_results])\n",
        "\n",
        "        results.append({\"Song Title\": song_title, \"Singer\": song_singer, \"Top 3 Similarities\": song_results})\n",
        "\n",
        "    # 결과 출력\n",
        "    for result in results:\n",
        "        print(f\"{result['Singer']} - {result['Song Title']}\")\n",
        "        for entry in result[\"Top 3 Similarities\"]:\n",
        "            print(f\"{entry['Singer']} - {entry['Title']} : Final Score {entry['Final Score']:.4f}\")\n",
        "            print(f\"  (Song-Song Similarity: {entry['Song-Song Similarity']:.4f}, User-Song Similarity: {entry['User-Song Similarity']:.4f})\")\n",
        "        print(\"-\" * 30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OcXZfOIwIpz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "# 가중치 값 설정\n",
        "gamma = 0.3\n",
        "\n",
        "# 초기 데이터 처리\n",
        "if choice == '2':\n",
        "    dissimilar_playlists = pd.merge(dissimilar_playlists, melon_emotions, left_on=\"title\", right_on=\"Title\", how=\"inner\")\n",
        "    dissimilar_playlists = dissimilar_playlists[[\"title\", \"Emotions\", \"singer\"]]\n",
        "\n",
        "    results = []\n",
        "    seen_songs = set()\n",
        "\n",
        "    # 사용자 감정 벡터\n",
        "    user_emotion_vector = np.array(final_user_emotions).reshape(1, -1)\n",
        "\n",
        "    for index, row in dissimilar_playlists.iterrows():\n",
        "        song_title = row[\"title\"]\n",
        "        song_singer = row[\"singer\"]\n",
        "        song_vector = np.array(row[\"Emotions\"]).reshape(1, -1)\n",
        "\n",
        "        song_results = []\n",
        "        for i, emotion_vec in enumerate(emotions):\n",
        "            emotion_title = melon_emotions.iloc[i][\"Title\"]\n",
        "            emotion_singer = melon_emotions.iloc[i][\"singer\"]\n",
        "            emotion_vec = np.array(emotion_vec).reshape(1, -1)\n",
        "\n",
        "            if (\n",
        "                emotion_title != song_title and\n",
        "                emotion_title not in dissimilar_playlists[\"title\"].values and\n",
        "                emotion_title not in seen_songs\n",
        "            ):\n",
        "                try:\n",
        "                    # 곡 간 유사도(Song-Song Similarity)\n",
        "                    song_song_similarity = cosine_similarity(song_vector, emotion_vec)[0][0]\n",
        "\n",
        "                    # 사용자 감정 벡터와의 반대 유사도(User-Song Dissimilarity)\n",
        "                    opposite_user_song_similarity = 1 - cosine_similarity(user_emotion_vector, emotion_vec)[0][0]\n",
        "\n",
        "                    # Final Score 계산\n",
        "                    final_score = gamma * song_song_similarity + (1 - gamma) * opposite_user_song_similarity\n",
        "\n",
        "                    song_results.append({\n",
        "                        \"Title\": emotion_title,\n",
        "                        \"Singer\": emotion_singer,\n",
        "                        \"Song-Song Similarity\": song_song_similarity,\n",
        "                        \"User-Song Dissimilarity\": opposite_user_song_similarity,\n",
        "                        \"Final Score\": final_score\n",
        "                    })\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error with {song_title} vs {emotion_title}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Final Score를 기준으로 상위 3곡 선택 (값이 큰 곡이 반대되는 곡)\n",
        "        song_results = sorted(song_results, key=lambda x: x[\"Final Score\"], reverse=True)[:3]\n",
        "        seen_songs.update(entry[\"Title\"] for entry in song_results)\n",
        "\n",
        "        results.append({\"Song Title\": song_title, \"Singer\": song_singer, \"Top 3 Similarities\": song_results})\n",
        "\n",
        "    # 결과 출력\n",
        "    for result in results:\n",
        "        print(f\"{result['Singer']} - {result['Song Title']}\")\n",
        "        for entry in result[\"Top 3 Similarities\"]:\n",
        "            print(f\"{entry['Singer']} - {entry['Title']} : Final Score {entry['Final Score']:.4f}\")\n",
        "            print(f'  (Song-Song Similarity: {entry[\"Song-Song Similarity\"]:.4f}, User-Song Dissimilarity: {entry[\"User-Song Dissimilarity\"]:.4f})')\n",
        "        print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar6QrqVD8r68"
      },
      "source": [
        "# 15. 최종 플레이리스트 산출\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkjGW-J4YLjs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터프레임 변환을 위한 리스트 생성\n",
        "df_rows = []\n",
        "\n",
        "for result in results:\n",
        "    song_title = result['Song Title']\n",
        "    song_singer = result['Singer']\n",
        "    main_song_info = f\"{song_singer} - {song_title}\"\n",
        "\n",
        "    for entry in result[\"Top 3 Similarities\"]:\n",
        "        combined_info = f\"{entry['Singer']} - {entry['Title']}\"\n",
        "        df_rows.append({\"1st 추천 플레이리스트\": main_song_info, \"2nd 추천 플레이리스트\": combined_info})\n",
        "\n",
        "# 데이터프레임 생성\n",
        "final_music_playlist_recommendation = pd.DataFrame(df_rows)\n",
        "\n",
        "# 곡 제목 그룹화하여 첫 번째 행에만 곡 제목 표시\n",
        "final_music_playlist_recommendation[\"1st 추천 플레이리스트\"] = final_music_playlist_recommendation.groupby(\"1st 추천 플레이리스트\")[\"1st 추천 플레이리스트\"].transform(\n",
        "    lambda x: [x.iloc[0]] + [\"\"] * (len(x) - 1)\n",
        ")\n",
        "\n",
        "final_music_playlist_recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-fWTPxLPs-Y"
      },
      "source": [
        "# 16. 생성형 AI\n",
        "\n",
        "*api 비공개"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q6fjbB_MaUwo"
      },
      "outputs": [],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_ZMLSvccSXE"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "print(openai.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIKh9XY0O1PC"
      },
      "source": [
        "### 1. 감정 표현 일러스트 캐릭터 이미지 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JcXYem4aISI"
      },
      "outputs": [],
      "source": [
        "sentence = '요즘 마음이 너무 무겁고 답답해. 가끔은 아무것도 하고 싶지 않고 그냥 누워만 있고 싶어. 특히 어제 친구와의 작은 말다툼이 계속 마음에 남아서 그런지, 내가 뭔가 잘못했나 싶은 생각이 들더라. 그 친구가 얼마나 소중한지 알기에 더 마음이 아프고 미안한데, 사과를 하려고 하면 무슨 말을 해야 할지 몰라서 주저하게 돼. 나도 내 마음이 왜 이러는지 모르겠어.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "418u6w98Pwlk"
      },
      "outputs": [],
      "source": [
        "emotion_labels = ['기쁨', '즐거움', '사랑', '분노', '우울', '외로움']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8tzQJw2wP06z"
      },
      "outputs": [],
      "source": [
        "sentence_emotions = [0.00859011, 0.0011389 , 0.00697977, 0.06076408, 0.76232639, 0.16020081]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW6eNEo9BhGy"
      },
      "outputs": [],
      "source": [
        "def get_dominant_emotion(sentence_emotions, emotion_labels):\n",
        "    \"\"\"\n",
        "    감정 벡터값에서 가장 높은 값을 가진 감정의 라벨을 반환하는 함수.\n",
        "\n",
        "    Args:\n",
        "        sentence_emotions (list of float): 감정 벡터값 리스트.\n",
        "        emotion_labels (list of str): 각 감정 벡터에 대응되는 감정 라벨 리스트.\n",
        "\n",
        "    Returns:\n",
        "        str: 가장 높은 감정 벡터값에 대응하는 감정 라벨.\n",
        "    \"\"\"\n",
        "    # 가장 높은 감정 벡터값의 인덱스 찾기\n",
        "    max_index = np.argmax(sentence_emotions)\n",
        "\n",
        "    # 해당 인덱스의 감정 라벨 반환\n",
        "    return emotion_labels[max_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec0Y_0UgPoNs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dominant_emotion = get_dominant_emotion(sentence_emotions, emotion_labels)\n",
        "print(dominant_emotion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDa3mn-2YxQQ"
      },
      "outputs": [],
      "source": [
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtNtYh3BYzSF"
      },
      "outputs": [],
      "source": [
        "dominant_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dFGrwH_QRvz"
      },
      "outputs": [],
      "source": [
        "# 이미지 생성 요청\n",
        "response = openai.Image.create(\n",
        "    model=\"dall-e-3\",  # 최신 DALL·E 모델 사용\n",
        "    prompt=(\n",
        "        f\"{sentence}을 반영해서 {dominant_emotion} 감정을 표현하는 3D 스타일의 일러스트 캐릭터를 그려줘. \"\n",
        "        \"캐릭터는 부드럽고 둥근 디자인에 표정이 감정을 잘 드러내야 해. \"\n",
        "        \"감정을 시각적으로 표현할 수 있는 소품이나 작은 상징 (예: 분노는 불꽃)을 포함해줘. \"\n",
        "        \"감정의 분위기를 반영하는 선명하고 깨끗한 색상을 사용하고, 캐릭터가 역동적이고 재미있는 자세를 취할 수 있도록 해줘. \"\n",
        "        \"이미지에는 하나의 캐릭터만 나오게 해줘.\"\n",
        "        \"배경은 단순하고 밝은 색상으로 설정해서 캐릭터가 강조될 수 있도록 해줘.\"\n",
        "    ),\n",
        "    size=\"1024x1024\",\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# 생성된 이미지 URL 출력\n",
        "image_url = response.data[0].url\n",
        "print(\"Generated Image URL:\", image_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d90l7NRMef3P"
      },
      "outputs": [],
      "source": [
        "response = openai.Image.create(\n",
        "    model=\"dall-e-3\",\n",
        "    prompt=(\n",
        "        f\"{sentence} 내용을 바탕으로, 어린아이가 그린 듯한 파스텔 톤의 그림 일기를 만들어줘. \"\n",
        "        \"그림은 색연필이나 크레용을 사용한 것처럼 부드럽고 질감이 느껴지도록 표현해줘. \"\n",
        "        \"그림은 단순하면서도 감정을 잘 표현해야 하고, 글자나 텍스트는 일절 포함하지 말아줘. \"\n",
        "        \"배경은 심플하고 깔끔하게 처리해서 주요 내용이 강조될 수 있도록 해줘.\"\n",
        "    ),\n",
        "    size=\"1024x1024\",\n",
        "    n=1\n",
        ")\n",
        "\n",
        "image_url = response.data[0].url\n",
        "print(image_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhji9U8SQ1sJ"
      },
      "source": [
        "### 2. 일기 모멘텀 : 추천된 일기 콘텐츠와 함께하는 오늘의 회고"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TVeNrUT4iZT"
      },
      "outputs": [],
      "source": [
        "# 챗봇 스타일 옵션 제공\n",
        "options = {\n",
        "    1: \"친근한\",  # 부담 없이 편안한 친구 같은 대화\n",
        "    2: \"MZ세대\",  # 트렌디한 유행어와 짧고 재미있는 대화\n",
        "    3: \"유머러스한\",  # 웃음과 위트를 담은 대화\n",
        "    4: \"현명한 조언자\", # 따뜻한 격려 및 조언을 담은 대화\n",
        "    5: \"문학적 감성\",  # 시적이고 은유적인 표현을 담은 대화\n",
        "}\n",
        "\n",
        "# 옵션 출력\n",
        "print(\"당신의 일기를 함께 작성하고 싶은 챗봇 스타일을 선택하세요:\")\n",
        "for key, value in options.items():\n",
        "    print(f\"{key}. {value}\")\n",
        "\n",
        "# 사용자 입력 처리\n",
        "while True:\n",
        "    try:\n",
        "        # 사용자 번호 입력\n",
        "        selection = int(input(\"번호를 입력하세요: \"))\n",
        "\n",
        "        # 유효한 번호인지 확인\n",
        "        if selection in options:\n",
        "            style = options[selection]\n",
        "            print(f\"선택한 스타일: {style}\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"유효한 번호를 입력해주세요.\")\n",
        "    except ValueError:\n",
        "        print(\"숫자를 입력해주세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DspzCOPit6Py"
      },
      "outputs": [],
      "source": [
        "# ChatGPT API를 활용한 일기 코멘트와 모멘텀 추천 로직\n",
        "\n",
        "# 1. 일기에 대한 짧은 코멘트 생성\n",
        "system_prompt_comment = f\"너는 사용자가 작성한 일기에 대해 짧은 코멘트를 제공하는 {style} 챗봇이야.\"\n",
        "response_comment = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt_comment},\n",
        "            {\"role\": \"user\", \"content\": sentence}],\n",
        "        temperature = 1,\n",
        "        presence_penalty = 1,\n",
        "        frequency_penalty = 1,\n",
        "        n = 1\n",
        "    )\n",
        "\n",
        "# 코멘트 출력\n",
        "comment = response_comment.choices[0].message.content\n",
        "print(comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xxe2DAaVoPR"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "system_prompt_momentum = f\"너는 일기 제목과 오늘 하루를 돌아볼 수 있도록 실질적인 일기 주제를 4-5개 정도 추천해주는 {style}의 일기 컨텐츠를 제공하는 챗봇이야.\"\n",
        "\n",
        "## 초기 사용자 일기\n",
        "\n",
        "# 첫 번째 대화\n",
        "def get_initial_response(sentence):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt_momentum},\n",
        "                {\"role\": \"user\", \"content\": sentence}\n",
        "            ],\n",
        "            temperature = 1,\n",
        "            presence_penalty = 1,\n",
        "            frequency_penalty = 1,\n",
        "            n = 1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"오류가 발생했습니다: {e}\"\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def get_current_date():\n",
        "    today = datetime.now()\n",
        "    return today.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "get_initial_response(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXGGTOB59yKE"
      },
      "source": [
        "#17. Gradio (hugging face 배포 ver.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O2B6-sba-JL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
        "import gradio as gr\n",
        "import openai\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUIHdYhVzSY9"
      },
      "outputs": [],
      "source": [
        "# gradio final ver ----------------------------\n",
        "\n",
        "###### 기본 설정 ######\n",
        "# OpenAI API 키 설정\n",
        "\n",
        "\n",
        "# 모델 및 프로세서 로드\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "model_clip = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# 예측 레이블\n",
        "labels = ['a photo of a happy face', 'a photo of a joyful face', 'a photo of a loving face',\n",
        "          'a photo of an angry face', 'a photo of a melancholic face', 'a photo of a lonely face']\n",
        "\n",
        "###### 얼굴 감정 벡터 예측 함수 ######\n",
        "def predict_face_emotion(image):\n",
        "    # 이미지가 None이거나 잘못된 경우\n",
        "    if image is None:\n",
        "        return np.zeros(len(labels))  # 빈 벡터 반환\n",
        "\n",
        "    # PIL 이미지를 RGB로 변환\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    # CLIP 모델의 processor를 이용한 전처리\n",
        "    inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # pixel_values가 4차원인지 확인 후 강제 변환\n",
        "    pixel_values = inputs[\"pixel_values\"]  # (batch_size, channels, height, width)\n",
        "\n",
        "    # CLIP 모델 예측: forward에 올바른 입력 전달\n",
        "    with torch.no_grad():\n",
        "        outputs = model_clip(pixel_values=pixel_values, input_ids=inputs[\"input_ids\"])\n",
        "\n",
        "    # 확률값 계산\n",
        "    probs = outputs.logits_per_image.softmax(dim=1)[0]\n",
        "    return probs.numpy()\n",
        "\n",
        "###### 텍스트 감정 벡터 예측 함수 ######\n",
        "sentence_emotions = []\n",
        "\n",
        "def predict_text_emotion(predict_sentence):\n",
        "\n",
        "    if not isinstance(predict_sentence, str):\n",
        "        predict_sentence = str(predict_sentence)\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=1, num_workers=5)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        for i in out:\n",
        "            logits = i.detach().cpu().numpy()\n",
        "            emotions = [value.item() for value in i]\n",
        "            sentence_emotions.append(emotions)\n",
        "    return sentence_emotions[0]  # 최종 리스트 반환\n",
        "\n",
        "###### 최종 감정 벡터 계산 ######\n",
        "def generate_final_emotion_vector(diary_input, image_input):\n",
        "    # 텍스트 감정 벡터 예측\n",
        "    text_vector = predict_text_emotion(diary_input)\n",
        "    # 얼굴 감정 벡터 예측\n",
        "    image_vector = predict_face_emotion(image_input)\n",
        "    text_vector = np.array(text_vector, dtype=float)\n",
        "    image_vector = np.array(image_vector, dtype=float)\n",
        "\n",
        "    print(text_vector)\n",
        "    print(image_vector)\n",
        "\n",
        "    # 최종 감정 벡터 가중치 적용\n",
        "    return (text_vector * 0.7) + (image_vector * 0.3)\n",
        "\n",
        "####### 코사인 유사도 함수 ######\n",
        "def cosine_similarity_fn(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return np.nan  # 제로 벡터인 경우 NaN 반환\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "\n",
        "####### 이미지 다운로드 함수 (PIL 객체 반환) ######\n",
        "def download_image(image_url):\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        response.raise_for_status()\n",
        "        return Image.open(requests.get(image_url, stream=True).raw)\n",
        "    except Exception as e:\n",
        "        print(f\"이미지 다운로드 오류: {e}\")\n",
        "        return None\n",
        "\n",
        "# 스타일 옵션\n",
        "options = {\n",
        "    1: \"🌼 친근한\",\n",
        "    2: \"🔥 트렌디한 MZ세대\",\n",
        "    3: \"😄 유머러스한 장난꾸러기\",\n",
        "    4: \"🧘 차분한 명상가\",\n",
        "    5: \"🎨 창의적인 예술가\",\n",
        "}\n",
        "\n",
        "# 일기 분석 함수\n",
        "def chatbot_diary_with_image(style_option, diary_input, image_input, playlist_input):\n",
        "\n",
        "    style = options.get(int(style_option.split('.')[0]), \"🌼 친근한\")\n",
        "\n",
        "    # GPT 응답 (일기 코멘트)\n",
        "    try:\n",
        "        response_comment = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": f\"너는 {style} 챗봇이야.\"}, {\"role\": \"user\", \"content\": diary_input}],\n",
        "        )\n",
        "        comment = response_comment.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        comment = f\"💬 오류: {e}\"\n",
        "\n",
        "    # GPT 기반 일기 주제 추천\n",
        "    try:\n",
        "        topics = get_initial_response(style_option, diary_input)\n",
        "    except Exception as e:\n",
        "        topics = f\"📝 주제 추천 오류: {e}\"\n",
        "\n",
        "    # DALL·E 3 이미지 생성 요청 (3D 스타일 캐릭터)\n",
        "    try:\n",
        "        response = openai.Image.create(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=(\n",
        "                  f\"{diary_input}를 반영해서 감정을 표현하는 3D 스타일의 일러스트 캐릭터를 그려줘. \"\n",
        "                  \"캐릭터는 부드럽고 둥근 디자인에 표정이 감정을 잘 드러내야 해. \"\n",
        "                  \"감정을 시각적으로 표현할 수 있는 소품이나 작은 상징을 포함해줘. \"\n",
        "                  \"감정의 분위기를 반영하는 선명하고 깨끗한 색상을 사용하고, 캐릭터가 역동적이고 재미있는 자세를 취할 수 있도록 해줘. \"\n",
        "                  \"이미지에는 하나의 캐릭터만 나오게 해줘.\"\n",
        "                  \"배경은 단순하고 밝은 색상으로 설정해서 캐릭터가 강조될 수 있도록 해줘.\"\n",
        "            ),\n",
        "            size=\"1024x1024\",\n",
        "            n=1\n",
        "        )\n",
        "        # URL 가져오기 및 다운로드\n",
        "        image_url = response['data'][0]['url']\n",
        "        print(f\"Generated Image URL: {image_url}\")  # URL 확인\n",
        "        image = download_image(image_url)\n",
        "    except Exception as e:\n",
        "        print(f\"이미지 생성 오류: {e}\")  # 오류 상세 출력\n",
        "        image = None\n",
        "\n",
        "    # 사용자 최종 감정 벡터\n",
        "    final_user_emotions = generate_final_emotion_vector(diary_input,image_input)\n",
        "\n",
        "    # 각 노래에 대한 코사인 유사도 계산\n",
        "    similarities = [cosine_similarity_fn(final_user_emotions, song_vec) for song_vec in emotions]\n",
        "\n",
        "    #유효한 유사도 필터링\n",
        "    valid_indices = [i for i, sim in enumerate(similarities) if not np.isnan(sim)]\n",
        "    filtered_similarities = [similarities[i] for i in valid_indices]\n",
        "\n",
        "    recommendations = np.argsort(filtered_similarities)[::-1]  # 높은 유사도 순으로 정렬\n",
        "    results_df = pd.DataFrame({\n",
        "    'Singer' : melon_emotions['singer'].iloc[recommendations].values,\n",
        "    'title' : melon_emotions['Title'].iloc[recommendations].values,\n",
        "    'genre' : melon_emotions['genre'].iloc[recommendations].values,\n",
        "    'Cosine Similarity': [similarities[idx] for idx in recommendations]\n",
        "    })\n",
        "\n",
        "    # 가중치 값 설정\n",
        "    gamma = 0.3\n",
        "\n",
        "    similar_playlists = results_df.head(5)\n",
        "    similar_playlists = pd.merge(similar_playlists, melon_emotions, left_on=\"title\", right_on=\"Title\", how=\"inner\")\n",
        "    similar_playlists = similar_playlists[[\"title\", \"Emotions\", \"singer\"]]\n",
        "\n",
        "    dissimilar_playlists = results_df.tail(5)\n",
        "    dissimilar_playlists = pd.merge(dissimilar_playlists, melon_emotions, left_on=\"title\", right_on=\"Title\", how=\"inner\")\n",
        "    dissimilar_playlists = dissimilar_playlists[[\"title\", \"Emotions\", \"singer\"]]\n",
        "\n",
        "    #감정과 유사한 플레이리스트\n",
        "    if playlist_input == '비슷한':\n",
        "      results = []\n",
        "      seen_songs = set(similar_playlists[\"title\"].values)  # 초기 seen_songs에 similar_playlists의 곡들을 추가\n",
        "\n",
        "      # 사용자 감정 벡터\n",
        "      user_emotion_vector = generate_final_emotion_vector(diary_input, image_input).reshape(1, -1)\n",
        "\n",
        "      for index, row in similar_playlists.iterrows():\n",
        "          song_title = row[\"title\"]\n",
        "          song_singer = row[\"singer\"]\n",
        "          song_vector = np.array(row[\"Emotions\"]).reshape(1, -1)\n",
        "\n",
        "          song_results = []\n",
        "          for i, emotion_vec in enumerate(emotions):\n",
        "              emotion_title = melon_emotions.iloc[i][\"Title\"]\n",
        "              emotion_singer = melon_emotions.iloc[i][\"singer\"]\n",
        "              emotion_vec = np.array(emotion_vec).reshape(1, -1)\n",
        "\n",
        "              # similar_playlists에 있는 곡과 seen_songs에 있는 곡은 제외\n",
        "              if (\n",
        "                  emotion_title != song_title and\n",
        "                  emotion_title not in seen_songs\n",
        "              ):\n",
        "                  try:\n",
        "                      # 곡 간 유사도(Song-Song Similarity)\n",
        "                      song_song_similarity = cosine_similarity(song_vector, emotion_vec)[0][0]\n",
        "\n",
        "                      # 사용자 감정 벡터와의 유사도(User-Song Similarity)\n",
        "                      user_song_similarity = cosine_similarity(user_emotion_vector, emotion_vec)[0][0]\n",
        "\n",
        "                      # Final Score 계산\n",
        "                      final_score = gamma * song_song_similarity + (1 - gamma) * user_song_similarity\n",
        "\n",
        "                      song_results.append({\n",
        "                          \"Title\": emotion_title,\n",
        "                          \"Singer\": emotion_singer,\n",
        "                          \"Song-Song Similarity\": song_song_similarity,\n",
        "                          \"User-Song Similarity\": user_song_similarity,\n",
        "                          \"Final Score\": final_score\n",
        "                      })\n",
        "                  except ValueError as e:\n",
        "                      print(f\"Error with {song_title} vs {emotion_title}: {e}\")\n",
        "                      continue\n",
        "\n",
        "          # Final Score를 기준으로 상위 3곡 선택\n",
        "          song_results = sorted(song_results, key=lambda x: x[\"Final Score\"], reverse=True)[:3]\n",
        "          seen_songs.update([entry[\"Title\"] for entry in song_results])\n",
        "\n",
        "          results.append({\"Song Title\": song_title, \"Singer\": song_singer, \"Top 3 Similarities\": song_results})\n",
        "\n",
        "      # 결과 출력\n",
        "      for result in results:\n",
        "          print(f\"{result['Singer']} - {result['Song Title']}\")\n",
        "          for entry in result[\"Top 3 Similarities\"]:\n",
        "              print(f\"{entry['Singer']} - {entry['Title']} : Final Score {entry['Final Score']:.4f}\")\n",
        "              print(f\"  (Song-Song Similarity: {entry['Song-Song Similarity']:.4f}, User-Song Similarity: {entry['User-Song Similarity']:.4f})\")\n",
        "          print(\"-\" * 30)\n",
        "\n",
        "    #반대 플레이리스트\n",
        "    if playlist_input == '상반된':\n",
        "      results = []\n",
        "      seen_songs = set()\n",
        "\n",
        "      # 사용자 감정 벡터\n",
        "      user_emotion_vector = generate_final_emotion_vector(diary_input, image_input).reshape(1, -1)\n",
        "\n",
        "      for index, row in dissimilar_playlists.iterrows():\n",
        "          song_title = row[\"title\"]\n",
        "          song_singer = row[\"singer\"]\n",
        "          song_vector = np.array(row[\"Emotions\"]).reshape(1, -1)\n",
        "\n",
        "          song_results = []\n",
        "          for i, emotion_vec in enumerate(emotions):\n",
        "              emotion_title = melon_emotions.iloc[i][\"Title\"]\n",
        "              emotion_singer = melon_emotions.iloc[i][\"singer\"]\n",
        "              emotion_vec = np.array(emotion_vec).reshape(1, -1)\n",
        "\n",
        "              if (\n",
        "                  emotion_title != song_title and\n",
        "                  emotion_title not in dissimilar_playlists[\"title\"].values and\n",
        "                  emotion_title not in seen_songs\n",
        "              ):\n",
        "                  try:\n",
        "                      # 곡 간 유사도(Song-Song Similarity)\n",
        "                      song_song_similarity = cosine_similarity(song_vector, emotion_vec)[0][0]\n",
        "\n",
        "                      # 사용자 감정 벡터와의 반대 유사도(User-Song Dissimilarity)\n",
        "                      opposite_user_song_similarity = 1 - cosine_similarity(user_emotion_vector, emotion_vec)[0][0]\n",
        "\n",
        "                      # Final Score 계산\n",
        "                      final_score = gamma * song_song_similarity + (1 - gamma) * opposite_user_song_similarity\n",
        "\n",
        "                      song_results.append({\n",
        "                          \"Title\": emotion_title,\n",
        "                          \"Singer\": emotion_singer,\n",
        "                          \"Song-Song Similarity\": song_song_similarity,\n",
        "                          \"User-Song Dissimilarity\": opposite_user_song_similarity,\n",
        "                          \"Final Score\": final_score\n",
        "                      })\n",
        "                  except ValueError as e:\n",
        "                      print(f\"Error with {song_title} vs {emotion_title}: {e}\")\n",
        "                      continue\n",
        "\n",
        "          # Final Score를 기준으로 상위 3곡 선택 (값이 큰 곡이 반대되는 곡)\n",
        "          song_results = sorted(song_results, key=lambda x: x[\"Final Score\"], reverse=True)[:3]\n",
        "          seen_songs.update(entry[\"Title\"] for entry in song_results)\n",
        "\n",
        "          results.append({\"Song Title\": song_title, \"Singer\": song_singer, \"Top 3 Similarities\": song_results})\n",
        "\n",
        "      # 결과 출력\n",
        "      for result in results:\n",
        "          print(f\"{result['Singer']} - {result['Song Title']}\")\n",
        "          for entry in result[\"Top 3 Similarities\"]:\n",
        "              print(f\"{entry['Singer']} - {entry['Title']} : Final Score {entry['Final Score']:.4f}\")\n",
        "              print(f'  (Song-Song Similarity: {entry[\"Song-Song Similarity\"]:.4f}, User-Song Dissimilarity: {entry[\"User-Song Dissimilarity\"]:.4f})')\n",
        "          print(\"-\" * 30)\n",
        "    # 데이터프레임 변환을 위한 리스트 생성\n",
        "    df_rows = []\n",
        "\n",
        "    for result in results:\n",
        "        song_title = result['Song Title']\n",
        "        song_singer = result['Singer']\n",
        "        main_song_info = f\"{song_singer} - {song_title}\"\n",
        "\n",
        "        for entry in result[\"Top 3 Similarities\"]:\n",
        "            combined_info = f\"{entry['Singer']} - {entry['Title']}\"\n",
        "            df_rows.append({\"1st 추천 플레이리스트\": main_song_info, \"2nd 추천 플레이리스트\": combined_info})\n",
        "\n",
        "    # 데이터프레임 생성\n",
        "    final_music_playlist_recommendation = pd.DataFrame(df_rows)\n",
        "\n",
        "    # 곡 제목 그룹화하여 첫 번째 행에만 곡 제목 표시\n",
        "    final_music_playlist_recommendation[\"1st 추천 플레이리스트\"] = final_music_playlist_recommendation.groupby(\"1st 추천 플레이리스트\")[\"1st 추천 플레이리스트\"].transform(\n",
        "        lambda x: [x.iloc[0]] + [\"\"] * (len(x) - 1)\n",
        "    )\n",
        "\n",
        "    return final_music_playlist_recommendation, comment, topics, image\n",
        "\n",
        "# 일기 주제 추천 함수\n",
        "def get_initial_response(style, sentence):\n",
        "    style = options.get(int(style.split('.')[0]), \"🌼 친근한\")\n",
        "    system_prompt_momentum = (\n",
        "        f\"너는 {style}의 챗봇이야. 사용자가 작성한 일기를 바탕으로 생각을 정리하고 내면을 돌아볼 수 있도록 \"\n",
        "        \"도와주는 구체적인 일기 콘텐츠나 질문 4-5개를 추천해줘.\"\n",
        "    )\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt_momentum},\n",
        "                {\"role\": \"user\", \"content\": sentence}\n",
        "            ],\n",
        "            temperature=1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"📝 주제 추천 오류: {e}\"\n",
        "\n",
        "# Gradio 인터페이스\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# ✨ 스마트 감정 일기 서비스 ✨\\n\\n 오늘의 하루를 기록하면, 그에 맞는 플레이리스트와 일기 회고 콘텐츠를 자동으로 생성해드립니다!\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            chatbot_style = gr.Radio(\n",
        "                choices=[f\"{k}. {v}\" for k, v in options.items()],\n",
        "                label=\"🤖 원하는 챗봇 스타일 선택\"\n",
        "            )\n",
        "            diary_input = gr.Textbox(label=\"📜 오늘의 하루 기록하기\", placeholder=\"ex)오늘 소풍가서 맛있는 걸 많이 먹어서 엄청 신났어\")\n",
        "            image_input = gr.Image(type=\"pil\", label=\"📷 얼굴 표정 사진 업로드\")\n",
        "            playlist_input = gr.Radio([\"비슷한\", \"상반된\"], label=\"🎧 오늘의 감정과 ㅇㅇ되는 플레이리스트 추천 받기\")\n",
        "            submit_btn = gr.Button(\"🚀 분석 시작\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_playlist = gr.Dataframe(label=\"🎧 추천 플레이리스트 \")\n",
        "            output_comment = gr.Textbox(label=\"💬 AI 코멘트\")\n",
        "            output_topics = gr.Textbox(label=\"📝 추천 일기 콘텐츠\")\n",
        "            output_image = gr.Image(label=\"🖼️ 생성된 오늘의 감정 캐릭터\", type=\"pil\", width=512, height=512)\n",
        "\n",
        "    # 버튼 클릭 이벤트 연결\n",
        "    submit_btn.click(\n",
        "        fn=chatbot_diary_with_image,\n",
        "        inputs=[chatbot_style, diary_input, image_input, playlist_input],\n",
        "        outputs=[output_playlist, output_comment, output_topics, output_image]\n",
        "    )\n",
        "\n",
        "# 앱 실행\n",
        "app.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1BkgfHmYznOZ",
        "0FzuB_WNzf0S",
        "S8eDT5n33dCX",
        "oqr04ZvX5M4i",
        "Ax6oEA1yoOSC",
        "5ijwhvJNBI1C",
        "jTxSQ-2EAyih",
        "It_kI5HKEef0",
        "40Gya7eoFZW3",
        "opQsMp3oDB5Q",
        "SiLMtZfyV7-S",
        "9-jfn_m2xl7Q"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}